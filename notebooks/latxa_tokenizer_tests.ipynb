{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4ce9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"HiTZ/latxa-7b-v1.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4477d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 32000\n",
      "Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
      "Special token IDs: {'bos_token': 1, 'eos_token': 2, 'unk_token': 0}\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size\n",
    "print(\"Vocabulary size:\", tokenizer.vocab_size)\n",
    "\n",
    "# Special tokens (e.g., padding, unknown, BOS, EOS)\n",
    "print(\"Special tokens:\", tokenizer.special_tokens_map)\n",
    "\n",
    "# Example: IDs of special tokens\n",
    "print(\"Special token IDs:\", {k: tokenizer.convert_tokens_to_ids(v) \n",
    "                             for k,v in tokenizer.special_tokens_map.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb7e452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [1, 382, 17400, 2518, 594, 19933, 564, 698, 21722, 616, 1572, 3805, 277, 1039, 1146, 29991]\n",
      "Decoded text: <s> Euskara adimen arttifizialera iritsi da!\n"
     ]
    }
   ],
   "source": [
    "text = \"Euskara adimen arttifizialera iritsi da!\"\n",
    "\n",
    "# Encode text â†’ token IDs\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(\"Token IDs:\", token_ids)\n",
    "\n",
    "# Decode back to text\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(\"Decoded text:\", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cb0220d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded, 32000 tokens found.\n",
      "Vocabulary saved to latxa-7b-v1.1-vocab.tsv\n"
     ]
    }
   ],
   "source": [
    "# Save vocabulary to TSV file\n",
    "vocab = tokenizer.get_vocab()  # dict: token -> id\n",
    "print(f\"Vocabulary loaded, {len(vocab)} tokens found.\")\n",
    "vocab_sorted = sorted(vocab.items(), key=lambda x: x[1])\n",
    "OUTPUT_FILE = \"latxa-7b-v1.1-vocab.tsv\"\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"token\\tid\\n\")\n",
    "    for token, idx in vocab_sorted:\n",
    "        f.write(f\"{token}\\t{idx}\\n\")\n",
    "\n",
    "print(f\"Vocabulary saved to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95de244d",
   "metadata": {},
   "source": [
    "Tokens per word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4ea4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def avg_tokens_per_word(sentences):\n",
    "    token_counts = []\n",
    "    word_counts = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        words = sent.split()\n",
    "\n",
    "        token_counts.append(len(tokens))\n",
    "        word_counts.append(len(words))\n",
    "\n",
    "    avg_tokens_word = sum(token_counts) / sum(word_counts)\n",
    "    avg_tokens_sentence = sum(token_counts) / len(sentences)\n",
    "\n",
    "    return avg_tokens_word, avg_tokens_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "772e586c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average tokens per word: 3.086\n",
      "Average tokens per sentence: 43.36\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"../data/eus_latn_data.csv\"\n",
    "df = pd.read_csv(data_dir)\n",
    "sentences = df['sentence'].tolist()\n",
    "\n",
    "avg_word, avg_sentence = avg_tokens_per_word(sentences)\n",
    "\n",
    "print(f\"Average tokens per word: {avg_word:.3f}\")\n",
    "print(f\"Average tokens per sentence: {avg_sentence:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc3e1ca",
   "metadata": {},
   "source": [
    "Latxa tokenizer vs. Llama2 tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
