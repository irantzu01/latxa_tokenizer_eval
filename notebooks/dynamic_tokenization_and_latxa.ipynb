{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c382fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../dynamic-tokenization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e93823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizations.dynamic_bpe import Dynamic_BPE\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "import torch\n",
    "from zett.utils import get_surface_form_matrix\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "866a0b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16d3e7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../dynamic-tokenization\")\n",
    "sys.path.append(\"../dynamic_tokenization_tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d2a7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Load Latxa model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HiTZ/latxa-7b-v1.2\")\n",
    "latxa_tokenizer = AutoTokenizer.from_pretrained(\"HiTZ/latxa-7b-v1.2\")\n",
    "print(\"Latxa model and tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da462804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hypernetwork\n",
    "hypernet = AutoModel.from_pretrained(\n",
    "    \"benjamin/zett-hypernetwork-Meta-Llama-3-8B-experimental\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "hypernet_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"benjamin/zett-hypernetwork-Meta-Llama-3-8B-experimental\"\n",
    ")\n",
    "\n",
    "# hypernet.config.hf_model_type = \"meta-llama/LlamaForCausalLM\"\n",
    "\n",
    "# orig_forward = hypernet.forward\n",
    "# def patched_forward(*args, **kwargs):\n",
    "#     out = orig_forward(*args, **kwargs)\n",
    "#     if isinstance(out, tuple) and len(out) == 2:\n",
    "#         return out[0], out[1], None\n",
    "#     return out\n",
    "# hypernet.forward = patched_forward\n",
    "\n",
    "# dynamic BPE init\n",
    "dynamic_bpe = Dynamic_BPE(\n",
    "    tokenizer=hypernet_tokenizer,\n",
    "    tokenizer_boundary=\"pretokens\",\n",
    ")\n",
    "\n",
    "# # fix required by Dynamic_BPE for tokenizer.model.config\n",
    "# hypernet_tokenizer.model = type(\"x\", (), {})()\n",
    "# hypernet_tokenizer.model.config = type(\"y\", (), {})()\n",
    "# hypernet_tokenizer.model.config.hf_model_type = \"meta-llama/LlamaForCausalLM\"\n",
    "\n",
    "print(\"Hypernetwork + tokenizer + Dynamic BPE ready.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79b57986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'candidates', 'answer'],\n",
      "        num_rows: 5169\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load EusProficiency dataset\n",
    "ds = load_dataset(\"HiTZ/EusProficiency\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c125d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicAugmenter:\n",
    "    \"\"\"\n",
    "    Runtime augmenter that:\n",
    "      - takes dynamic tokens (strings) produced per-batch,\n",
    "      - maps tokens already in latxa_vocab -> keep their ids,\n",
    "      - for new tokens: allocate new ids, predict embeddings with hypernet,\n",
    "        and write those embeddings into model's embedding matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, latxa_tokenizer, hypernet, hypernet_tokenizer, cache_limit=50000):\n",
    "        self.model = model\n",
    "        self.latxa_tokenizer = latxa_tokenizer\n",
    "        self.hypernet = hypernet.to(device)\n",
    "        self.hypernet_tokenizer = hypernet_tokenizer\n",
    "        # base HF vocab mapping (token string -> id)\n",
    "        self.vocab = latxa_tokenizer.get_vocab()\n",
    "        self.reverse_vocab = {v:k for k,v in self.vocab.items()}\n",
    "        self.base_vocab_size = len(self.vocab)\n",
    "        self.cache = OrderedDict()   # token_str -> token_id (preserve insertion order)\n",
    "        self.cache_embeddings = {}   # token_str -> (in_emb_tensor, out_emb_tensor)\n",
    "        self.cache_limit = cache_limit\n",
    "\n",
    "        # Ensure model on device\n",
    "        self.model.to(device)\n",
    "        # we will lazily resize embeddings when needed\n",
    "        self.current_vocab_size = self.base_vocab_size\n",
    "\n",
    "    def _ensure_capacity(self, n_new):\n",
    "        \"\"\"Resize model embeddings to accomodate n_new new ids.\"\"\"\n",
    "        new_size = self.current_vocab_size + n_new\n",
    "        if new_size == self.model.get_input_embeddings().num_embeddings:\n",
    "            return\n",
    "        # HF function to resize embeddings; preserves existing weights and creates new rows\n",
    "        self.model.resize_token_embeddings(new_size)\n",
    "        self.current_vocab_size = new_size\n",
    "\n",
    "    def _predict_embeddings_for_tokens(self, tokens_list):\n",
    "        \"\"\"\n",
    "        Use hypernet to predict embeddings for tokens_list (list of token strings).\n",
    "        Returns dict token -> (pred_in, pred_out) as torch tensors on device.\n",
    "        \"\"\"\n",
    "        # Tokenizer expects list of dicts for get_surface_form_matrix usage\n",
    "        batch_examples = [{\"text\": t} for t in tokens_list]\n",
    "\n",
    "        # Build surface forms matrix (the zett helper expects hypernet_tokenizer)\n",
    "        surfaces = get_surface_form_matrix(\n",
    "            [tokens_list],  # pass as list of list? the function in zett returns arrs; adapt if needed\n",
    "            maxlen=self.hypernet.config.hn_surface_maxlen,\n",
    "            tokenizer_to_use=self.hypernet_tokenizer\n",
    "        )[0]  # get first output if returns tuple\n",
    "\n",
    "        # Build source embeddings matrix from current model (concatenate in/out as in example)\n",
    "        src_emb = torch.cat([\n",
    "            self.model.get_input_embeddings().weight.data,\n",
    "            self.model.get_output_embeddings().weight.data\n",
    "        ], dim=1).to(device)\n",
    "\n",
    "        # surfaces -> hypernet prediction (adapt call to hypernet API)\n",
    "        with torch.no_grad():\n",
    "            pred_in, pred_out, _ = self.hypernet(\n",
    "                torch.from_numpy(surfaces).to(device),\n",
    "                source_embeddings=src_emb\n",
    "            )\n",
    "\n",
    "        # pred_in/out shape: (num_tokens, embedding_dim) etc. Convert to CPU/torch tensors\n",
    "        # Map predicted embeddings to tokens_list order\n",
    "        result = {}\n",
    "        for i, t in enumerate(tokens_list):\n",
    "            result[t] = (pred_in[i].detach().cpu(), pred_out[i].detach().cpu())\n",
    "\n",
    "        return result\n",
    "\n",
    "    def add_and_assign_new_tokens(self, new_token_strs):\n",
    "        \"\"\"\n",
    "        For token strings not in base vocab and not cached:\n",
    "           - predict embeddings with hypernet\n",
    "           - resize model embedding matrix\n",
    "           - write predicted embeddings to new rows\n",
    "        Return mapping token_str -> token_id (global)\n",
    "        \"\"\"\n",
    "        # Filter tokens not already in cache or vocab\n",
    "        to_create = [t for t in new_token_strs if (t not in self.vocab and t not in self.cache)]\n",
    "\n",
    "        if len(to_create) == 0:\n",
    "            # build mapping from cache/vocab for requested tokens\n",
    "            mapping = {}\n",
    "            for t in new_token_strs:\n",
    "                if t in self.vocab:\n",
    "                    mapping[t] = self.vocab[t]\n",
    "                else:\n",
    "                    mapping[t] = self.cache[t]\n",
    "            return mapping\n",
    "\n",
    "        # Predict embeddings with hypernet in chunks if many\n",
    "        CHUNK = 128\n",
    "        predicted = {}\n",
    "        for i in range(0, len(to_create), CHUNK):\n",
    "            chunk = to_create[i:i+CHUNK]\n",
    "            pred_chunk = self._predict_embeddings_for_tokens(chunk)\n",
    "            predicted.update(pred_chunk)\n",
    "\n",
    "        # Now allocate ids and ensure capacity\n",
    "        n_new = len(to_create)\n",
    "        self._ensure_capacity(n_new)\n",
    "\n",
    "        # Write embeddings into the model embedding matrix (on CPU then move)\n",
    "        # We will collect tensors to write to the new rows\n",
    "        input_emb = self.model.get_input_embeddings().weight.data  # on device\n",
    "        output_emb = self.model.get_output_embeddings().weight.data\n",
    "\n",
    "        # assign sequentially at the end\n",
    "        assigned = {}\n",
    "        next_id = self.current_vocab_size - n_new  # first index of newly created rows\n",
    "        # But careful: model.resize_token_embeddings sets current_vocab_size earlier. We stored it there.\n",
    "\n",
    "        # Actually recompute next_id as base + existing cache size\n",
    "        next_id = self.base_vocab_size + len([k for k in self.cache]) \n",
    "\n",
    "        for t in to_create:\n",
    "            in_emb_cpu, out_emb_cpu = predicted[t]  # CPU tensors\n",
    "            in_emb = in_emb_cpu.to(device)\n",
    "            out_emb = out_emb_cpu.to(device)\n",
    "            # new id\n",
    "            new_id = self.base_vocab_size + len(self.cache)\n",
    "            # Append to cache and embeddings\n",
    "            self.cache[t] = new_id\n",
    "            self.cache_embeddings[t] = (in_emb_cpu, out_emb_cpu)\n",
    "            # assign into model weights\n",
    "            # Note: input_emb and output_emb are tensors on device; assign by index\n",
    "            self.model.get_input_embeddings().weight.data[new_id, :] = in_emb\n",
    "            self.model.get_output_embeddings().weight.data[new_id, :] = out_emb\n",
    "            assigned[t] = new_id\n",
    "\n",
    "            # enforce cache limit\n",
    "            if len(self.cache) > self.cache_limit:\n",
    "                # pop oldest\n",
    "                old_token, old_id = self.cache.popitem(last=False)\n",
    "                self.cache_embeddings.pop(old_token, None)\n",
    "                # We do not reclaim embedding rows to keep indices stable (complex). Accept growth or restart.\n",
    "\n",
    "        # Build mapping for all requested tokens (new_token_strs)\n",
    "        mapping = {}\n",
    "        for t in new_token_strs:\n",
    "            if t in self.vocab:\n",
    "                mapping[t] = self.vocab[t]\n",
    "            else:\n",
    "                mapping[t] = self.cache[t]\n",
    "\n",
    "        # Update current_vocab_size if needed\n",
    "        self.current_vocab_size = self.model.get_input_embeddings().num_embeddings\n",
    "\n",
    "        return mapping\n",
    "\n",
    "    def tokens_to_ids(self, tokenized_batch):\n",
    "        \"\"\"\n",
    "        Convert a batch tokenized as lists of token strings (dynamic tokens)\n",
    "        into lists of token ids (ints) using base vocab + cache.\n",
    "        tokenized_batch: list[list[str]]\n",
    "        Returns: list[list[int]]\n",
    "        \"\"\"\n",
    "        # gather all unique tokens that are not in base vocab\n",
    "        uniques = set(t for seq in tokenized_batch for t in seq)\n",
    "        new_tokens = [t for t in uniques if t not in self.vocab]\n",
    "        # ensure they are created/assigned\n",
    "        mapping = self.add_and_assign_new_tokens(new_tokens)\n",
    "        # Now map sequences\n",
    "        out_ids = []\n",
    "        for seq in tokenized_batch:\n",
    "            ids = []\n",
    "            for t in seq:\n",
    "                if t in self.vocab:\n",
    "                    ids.append(self.vocab[t])\n",
    "                else:\n",
    "                    ids.append(self.cache[t])\n",
    "            out_ids.append(ids)\n",
    "        return out_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8aee1466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to normalize dynamic BPE tokens\n",
    "\n",
    "def normalize_dynbpe_tokens(batch_tokens):\n",
    "    cleaned = []\n",
    "    for seq in batch_tokens:\n",
    "        new_seq = []\n",
    "        for tok in seq:\n",
    "            # remove leading GPT whitespace marker if present\n",
    "            if tok.startswith(\"Ġ\"):\n",
    "                tok = tok[1:]\n",
    "\n",
    "            # if token is multi-character, split into characters\n",
    "            # because byte tokenizer expects char-level tokens\n",
    "            for ch in tok:\n",
    "                new_seq.append(ch)\n",
    "        cleaned.append(new_seq)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cbf2195",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DynamicAugmenter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m augmenter \u001b[38;5;241m=\u001b[39m \u001b[43mDynamicAugmenter\u001b[49m(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     latxa_tokenizer\u001b[38;5;241m=\u001b[39mlatxa_tokenizer,\n\u001b[1;32m      4\u001b[0m     hypernet\u001b[38;5;241m=\u001b[39mhypernet,\n\u001b[1;32m      5\u001b[0m     hypernet_tokenizer\u001b[38;5;241m=\u001b[39mhypernet_tokenizer,\n\u001b[1;32m      6\u001b[0m     cache_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sentences), BATCH_SIZE):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DynamicAugmenter' is not defined"
     ]
    }
   ],
   "source": [
    "augmenter = DynamicAugmenter(\n",
    "    model=model,\n",
    "    latxa_tokenizer=latxa_tokenizer,\n",
    "    hypernet=hypernet,\n",
    "    hypernet_tokenizer=hypernet_tokenizer,\n",
    "    cache_limit=50000\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "for i in range(0, len(sentences), BATCH_SIZE):\n",
    "    batch = sentences[i:i+BATCH_SIZE]\n",
    "    print(len(batch))\n",
    "    batch = [s for s in batch if s.strip() != \"\"]\n",
    "    print(len(batch))\n",
    "    examples = [{\"text\": s, \"pretokens\": s.split()} for s in batch]\n",
    "\n",
    "    # 1) Dynamic BPE returns token strings per sentence\n",
    "    dyn_tokens, _, _, _ = dynamic_bpe.tokenize_batch(\n",
    "        batch_examples=examples,\n",
    "        max_nr_merges=30,\n",
    "        mlm=True\n",
    "    )\n",
    "    # dyn_tokens is list[list[str]]\n",
    "    print(\"dyn_tokens example:\", dyn_tokens[0])\n",
    "    # Normalize tokens (remove Ġ, split multi-char into chars)\n",
    "    dyn_tokens = normalize_dynbpe_tokens(dyn_tokens)\n",
    "    print(\"Normalized dyn_tokens example:\", dyn_tokens[0])\n",
    "\n",
    "    # 2) Map tokens to ids, creating new embeddings as needed\n",
    "    batch_ids = augmenter.tokens_to_ids(dyn_tokens)  # list of lists\n",
    "    print(batch_ids[:2])\n",
    "\n",
    "    # 3) Convert to padded tensors for model\n",
    "    # pad with tokenizer.pad_token_id if you have one; else 0\n",
    "    pad_id = latxa_tokenizer.pad_token_id or latxa_tokenizer.eos_token_id\n",
    "    maxlen = max(len(x) for x in batch_ids)\n",
    "    input_ids = torch.full((len(batch_ids), maxlen), pad_id, dtype=torch.long, device=device)\n",
    "    attention_mask = torch.zeros_like(input_ids)\n",
    "    for r, seq in enumerate(batch_ids):\n",
    "        input_ids[r, :len(seq)] = torch.tensor(seq, dtype=torch.long, device=device)\n",
    "        attention_mask[r, :len(seq)] = 1\n",
    "\n",
    "    # 4) Run the model\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    # ... downstream evaluation ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c95eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dynamic Tokenization 3.11",
   "language": "python",
   "name": "dynamic_tokenization_311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
