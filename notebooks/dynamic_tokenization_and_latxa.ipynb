{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c382fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../dynamic-tokenization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e93823f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/irantzu/MASTER/WiSe25/Lab Rotation/dynamic-tokenization/dynamic_tokenization_311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tokenizations.dynamic_bpe import Dynamic_BPE\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "import torch\n",
    "from zett.utils import get_surface_form_matrix\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "866a0b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16d3e7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../dynamic-tokenization\")\n",
    "sys.path.append(\"../dynamic_tokenization_tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d2a7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Load Latxa model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HiTZ/latxa-7b-v1.2\")\n",
    "latxa_tokenizer = AutoTokenizer.from_pretrained(\"HiTZ/latxa-7b-v1.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da462804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hypernetwork\n",
    "hypernet = AutoModel.from_pretrained(\n",
    "    \"benjamin/zett-hypernetwork-Meta-Llama-3-8B-experimental\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "hypernet_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"benjamin/zett-hypernetwork-Meta-Llama-3-8B-experimental\"\n",
    ")\n",
    "\n",
    "# hypernet.config.hf_model_type = \"meta-llama/LlamaForCausalLM\"\n",
    "\n",
    "# orig_forward = hypernet.forward\n",
    "# def patched_forward(*args, **kwargs):\n",
    "#     out = orig_forward(*args, **kwargs)\n",
    "#     if isinstance(out, tuple) and len(out) == 2:\n",
    "#         return out[0], out[1], None\n",
    "#     return out\n",
    "# hypernet.forward = patched_forward\n",
    "\n",
    "# dynamic BPE init\n",
    "dynamic_bpe = Dynamic_BPE(\n",
    "    tokenizer=hypernet_tokenizer,\n",
    "    tokenizer_boundary=\"pretokens\",\n",
    ")\n",
    "\n",
    "# # fix required by Dynamic_BPE for tokenizer.model.config\n",
    "# hypernet_tokenizer.model = type(\"x\", (), {})()\n",
    "# hypernet_tokenizer.model.config = type(\"y\", (), {})()\n",
    "# hypernet_tokenizer.model.config.hf_model_type = \"meta-llama/LlamaForCausalLM\"\n",
    "\n",
    "# print(\"Hypernetwork + tokenizer + Dynamic BPE ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79b57986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EusCrawl dataset\n",
    "ds_euscrawl = load_dataset(\"HiTZ/latxa-corpus-v1.1\", 'euscrawl-v1.1')\n",
    "sentences = ds_euscrawl['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c125d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicAugmenter:\n",
    "    \"\"\"\n",
    "    Runtime augmenter that:\n",
    "      - takes dynamic tokens (strings) produced per-batch,\n",
    "      - maps tokens already in latxa_vocab -> keep their ids,\n",
    "      - for new tokens: allocate new ids, predict embeddings with hypernet,\n",
    "        and write those embeddings into model's embedding matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, latxa_tokenizer, hypernet, hypernet_tokenizer, cache_limit=50000):\n",
    "        self.model = model\n",
    "        self.latxa_tokenizer = latxa_tokenizer\n",
    "        self.hypernet = hypernet.to(device)\n",
    "        self.hypernet_tokenizer = hypernet_tokenizer\n",
    "        # base HF vocab mapping (token string -> id)\n",
    "        self.vocab = latxa_tokenizer.get_vocab()\n",
    "        self.reverse_vocab = {v:k for k,v in self.vocab.items()}\n",
    "        self.base_vocab_size = len(self.vocab)\n",
    "        self.cache = OrderedDict()   # token_str -> token_id (preserve insertion order)\n",
    "        self.cache_embeddings = {}   # token_str -> (in_emb_tensor, out_emb_tensor)\n",
    "        self.cache_limit = cache_limit\n",
    "\n",
    "        # Ensure model on device\n",
    "        self.model.to(device)\n",
    "        # we will lazily resize embeddings when needed\n",
    "        self.current_vocab_size = self.base_vocab_size\n",
    "\n",
    "    def _ensure_capacity(self, n_new):\n",
    "        \"\"\"Resize model embeddings to accomodate n_new new ids.\"\"\"\n",
    "        new_size = self.current_vocab_size + n_new\n",
    "        if new_size == self.model.get_input_embeddings().num_embeddings:\n",
    "            return\n",
    "        # HF function to resize embeddings; preserves existing weights and creates new rows\n",
    "        self.model.resize_token_embeddings(new_size)\n",
    "        self.current_vocab_size = new_size\n",
    "\n",
    "    def _predict_embeddings_for_tokens(self, tokens_list):\n",
    "        \"\"\"\n",
    "        Use hypernet to predict embeddings for tokens_list (list of token strings).\n",
    "        Returns dict token -> (pred_in, pred_out) as torch tensors on device.\n",
    "        \"\"\"\n",
    "        # Tokenizer expects list of dicts for get_surface_form_matrix usage\n",
    "        batch_examples = [{\"text\": t} for t in tokens_list]\n",
    "\n",
    "        # Build surface forms matrix (the zett helper expects hypernet_tokenizer)\n",
    "        surfaces = get_surface_form_matrix(\n",
    "            [tokens_list],  # pass as list of list? the function in zett returns arrs; adapt if needed\n",
    "            maxlen=self.hypernet.config.hn_surface_maxlen,\n",
    "            tokenizer_to_use=self.hypernet_tokenizer\n",
    "        )[0]  # get first output if returns tuple\n",
    "\n",
    "        # Build source embeddings matrix from current model (concatenate in/out as in example)\n",
    "        src_emb = torch.cat([\n",
    "            self.model.get_input_embeddings().weight.data,\n",
    "            self.model.get_output_embeddings().weight.data\n",
    "        ], dim=1).to(device)\n",
    "\n",
    "        # surfaces -> hypernet prediction (adapt call to hypernet API)\n",
    "        with torch.no_grad():\n",
    "            pred_in, pred_out, _ = self.hypernet(\n",
    "                torch.from_numpy(surfaces).to(device),\n",
    "                source_embeddings=src_emb\n",
    "            )\n",
    "\n",
    "        # pred_in/out shape: (num_tokens, embedding_dim) etc. Convert to CPU/torch tensors\n",
    "        # Map predicted embeddings to tokens_list order\n",
    "        result = {}\n",
    "        for i, t in enumerate(tokens_list):\n",
    "            result[t] = (pred_in[i].detach().cpu(), pred_out[i].detach().cpu())\n",
    "\n",
    "        return result\n",
    "\n",
    "    def add_and_assign_new_tokens(self, new_token_strs):\n",
    "        \"\"\"\n",
    "        For token strings not in base vocab and not cached:\n",
    "           - predict embeddings with hypernet\n",
    "           - resize model embedding matrix\n",
    "           - write predicted embeddings to new rows\n",
    "        Return mapping token_str -> token_id (global)\n",
    "        \"\"\"\n",
    "        # Filter tokens not already in cache or vocab\n",
    "        to_create = [t for t in new_token_strs if (t not in self.vocab and t not in self.cache)]\n",
    "\n",
    "        if len(to_create) == 0:\n",
    "            # build mapping from cache/vocab for requested tokens\n",
    "            mapping = {}\n",
    "            for t in new_token_strs:\n",
    "                if t in self.vocab:\n",
    "                    mapping[t] = self.vocab[t]\n",
    "                else:\n",
    "                    mapping[t] = self.cache[t]\n",
    "            return mapping\n",
    "\n",
    "        # Predict embeddings with hypernet in chunks if many\n",
    "        CHUNK = 128\n",
    "        predicted = {}\n",
    "        for i in range(0, len(to_create), CHUNK):\n",
    "            chunk = to_create[i:i+CHUNK]\n",
    "            pred_chunk = self._predict_embeddings_for_tokens(chunk)\n",
    "            predicted.update(pred_chunk)\n",
    "\n",
    "        # Now allocate ids and ensure capacity\n",
    "        n_new = len(to_create)\n",
    "        self._ensure_capacity(n_new)\n",
    "\n",
    "        # Write embeddings into the model embedding matrix (on CPU then move)\n",
    "        # We will collect tensors to write to the new rows\n",
    "        input_emb = self.model.get_input_embeddings().weight.data  # on device\n",
    "        output_emb = self.model.get_output_embeddings().weight.data\n",
    "\n",
    "        # assign sequentially at the end\n",
    "        assigned = {}\n",
    "        next_id = self.current_vocab_size - n_new  # first index of newly created rows\n",
    "        # But careful: model.resize_token_embeddings sets current_vocab_size earlier. We stored it there.\n",
    "\n",
    "        # Actually recompute next_id as base + existing cache size\n",
    "        next_id = self.base_vocab_size + len([k for k in self.cache]) \n",
    "\n",
    "        for t in to_create:\n",
    "            in_emb_cpu, out_emb_cpu = predicted[t]  # CPU tensors\n",
    "            in_emb = in_emb_cpu.to(device)\n",
    "            out_emb = out_emb_cpu.to(device)\n",
    "            # new id\n",
    "            new_id = self.base_vocab_size + len(self.cache)\n",
    "            # Append to cache and embeddings\n",
    "            self.cache[t] = new_id\n",
    "            self.cache_embeddings[t] = (in_emb_cpu, out_emb_cpu)\n",
    "            # assign into model weights\n",
    "            # Note: input_emb and output_emb are tensors on device; assign by index\n",
    "            self.model.get_input_embeddings().weight.data[new_id, :] = in_emb\n",
    "            self.model.get_output_embeddings().weight.data[new_id, :] = out_emb\n",
    "            assigned[t] = new_id\n",
    "\n",
    "            # enforce cache limit\n",
    "            if len(self.cache) > self.cache_limit:\n",
    "                # pop oldest\n",
    "                old_token, old_id = self.cache.popitem(last=False)\n",
    "                self.cache_embeddings.pop(old_token, None)\n",
    "                # We do not reclaim embedding rows to keep indices stable (complex). Accept growth or restart.\n",
    "\n",
    "        # Build mapping for all requested tokens (new_token_strs)\n",
    "        mapping = {}\n",
    "        for t in new_token_strs:\n",
    "            if t in self.vocab:\n",
    "                mapping[t] = self.vocab[t]\n",
    "            else:\n",
    "                mapping[t] = self.cache[t]\n",
    "\n",
    "        # Update current_vocab_size if needed\n",
    "        self.current_vocab_size = self.model.get_input_embeddings().num_embeddings\n",
    "\n",
    "        return mapping\n",
    "\n",
    "    def tokens_to_ids(self, tokenized_batch):\n",
    "        \"\"\"\n",
    "        Convert a batch tokenized as lists of token strings (dynamic tokens)\n",
    "        into lists of token ids (ints) using base vocab + cache.\n",
    "        tokenized_batch: list[list[str]]\n",
    "        Returns: list[list[int]]\n",
    "        \"\"\"\n",
    "        # gather all unique tokens that are not in base vocab\n",
    "        uniques = set(t for seq in tokenized_batch for t in seq)\n",
    "        new_tokens = [t for t in uniques if t not in self.vocab]\n",
    "        # ensure they are created/assigned\n",
    "        mapping = self.add_and_assign_new_tokens(new_tokens)\n",
    "        # Now map sequences\n",
    "        out_ids = []\n",
    "        for seq in tokenized_batch:\n",
    "            ids = []\n",
    "            for t in seq:\n",
    "                if t in self.vocab:\n",
    "                    ids.append(self.vocab[t])\n",
    "                else:\n",
    "                    ids.append(self.cache[t])\n",
    "            out_ids.append(ids)\n",
    "        return out_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8aee1466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to normalize dynamic BPE tokens\n",
    "\n",
    "def normalize_dynbpe_tokens(batch_tokens):\n",
    "    cleaned = []\n",
    "    for seq in batch_tokens:\n",
    "        new_seq = []\n",
    "        for tok in seq:\n",
    "            # remove leading GPT whitespace marker if present\n",
    "            if tok.startswith(\"Ġ\"):\n",
    "                tok = tok[1:]\n",
    "\n",
    "            # if token is multi-character, split into characters\n",
    "            # because byte tokenizer expects char-level tokens\n",
    "            for ch in tok:\n",
    "                new_seq.append(ch)\n",
    "        cleaned.append(new_seq)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cbf2195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "64\n",
      "dyn_tokens example: ['<s>', 'ĠZ', 'az', 'pi', 'Ġeta', 'Ġbed', 'er', 'at', 'zi', 'Ġurte', 'Ġarte', 'ko', 'Ġz', 'igor', 'Ġes', 'ka', 'er', 'ak', 'Ġham', 'ar', 'Ġgaz', 'ter', 'ent', 'zat', 'Ċ', 'Ċ', 'H', 'ern', 'an', 'iko', 'Ġgaz', 'teen', 'Ġaur', 'k', 'ako', 'ĠÂ«', 'ep', 'a', 'ik', 'eta', 'Ġpolitik', 'oa', 'Â»', 'Ġsal', 'atu', 'Ġdu', 'ĠEle', 'ak', '-', 'ek', ';', 'Ġlar', 'un', 'b', 'ater', 'ako', 'Ġmanifest', 'az', 'ior', 'a', 'Ġde', 'itu', 'Ġdu', 'Ċ', 'Ċ', 'Ir', 'ail', 'aren', 'Ġ', '18', 'tik', 'Ġ', '28', 'ra', 'Ġbit', 'art', 'ean', 'ĠHern', 'an', 'iko', 'Ġham', 'ar', 'Ġgaz', 'te', 'Ġep', 'ait', 'uko', 'Ġdituzte', 'ĠEsp', 'ain', 'i', 'ako', 'ĠA', 'uz', 'ite', 'gi', 'ĠNaz', 'ionale', 'an', '.', 'ĠZ', 'az', 'pi', 'Ġeta', 'Ġbed', 'er', 'at', 'zi', 'Ġurte', 'Ġarte', 'ko', 'Ġes', 'pet', 'xe', 'Ġzig', 'orr', 'ak', 'Ġesk', 'atu', 'Ġdituzte', 'Ġha', 'ient', 'zat', ',', 'ĠSeg', 'i', 'Ġgaz', 'te', 'Ġer', 'ak', 'und', 'eko', 'Ġk', 'ide', 'Ġiz', 'ate', 'a', 'Ġle', 'por', 'at', 'uta', '.', 'ĠEle', 'ak', 'Ġmug', 'im', 'end', 'uak', 'Ġsal', 'atu', 'Ġdu', 'Ġj', 'ardu', 'era', 'Ġpolit', 'iko', 'Ġeta', 'Ġso', 'zial', 'ag', 'atik', 'Ġep', 'ait', 'uko', 'Ġdituz', 't', 'ela', 'Ġham', 'arr', 'ak', ',', 'Ġeta', 'Ġmanifest', 'az', 'ior', 'a', 'Ġde', 'itu', 'Ġdu', 'ĠHern', 'an', 'in', 'Ġd', 'ator', 'ren', 'Ġlar', 'un', 'b', 'ater', 'ako', ',', 'Ġat', 'zo', 'Ġil', 'unt', 'ze', 'an', 'Ġeg', 'ind', 'ako', 'Ġpre', 'nt', 'sa', 'urre', 'ko', 'Ġj', 'end', 'ets', 'uan', '.', 'ĠD', 'ato', 'zen', 'Ġa', 'ste', 'etan', 'Ġep', 'ait', 'uko', 'Ġdituzte', ',', 'Ġbaina', 'Ġez', 'Ġda', 'Ġat', 'zo', 'Ġgo', 'ize', 'koa', 'Ġgaz', 'te', 'Ġhor', 'ien', 'Ġpol', 'izia', 'Ġet', 'xe', 'et', 'ako', ',', 'Ġes', 'pet', 'xe', 'et', 'ako', 'Ġeta', 'Ġep', 'a', 'ite', 'g', 'iet', 'ako', 'Ġib', 'il', 'era', '.', 'Ċ', 'Ċ', '200', '9', 'ko', 'Ġmart', 'xo', 'aren', 'Ġ', '31', 'n', 'Ġegin', 'Ġzuen', 'ĠEsp', 'ain', 'i', 'ako', 'ĠPol', 'izi', 'ak', 'ĠHern', 'ani', 'Ġeta', 'ĠUr', 'n', 'iet', 'ako', 'Ġgaz', 'te', 'Ġhor', 'ien', 'Ġaur', 'k', 'ako', 'Ġoper', 'azio', 'a', ',', 'ĠBalt', 'asar', 'ĠGar', 'zon', 'Ġep', 'a', 'ile', 'aren', 'Ġag', 'ind', 'uz', '.', 'ĠGo', 'iz', 'al', 'de', 'an', 'Ġegin', 'Ġz', 'uten', 'Ġoper', 'azio', 'a', ',', 'Ġeta', 'ĠÂ«', 'ego', 'era', 'Ġoso', 'Ġb', 'ort', 'itz', 'ak', 'Â»', 'Ġj', 'asan', 'Ġbehar', 'Ġizan', 'Ġz', 'it', 'uz', 't', 'ela', 'Ġjak', 'inar', 'azi', 'Ġdute', 'Ġgaz', 'te', 'ek', '.', 'ĠZ', 'ort', 'zi', 'Ġizan', 'Ġz', 'iren', 'Ġat', 'x', 'ilot', 'uak', ',', 'Ġeta', ',', 'Ġet', 'x', 'eko', 'Ġmi', 'ak', 'ete', 'z', 'Ġgain', ',', 'Ġb', 'ost', 'Ġeg', 'un', 'eko', 'Ġin', 'kom', 'unik', 'azio', 'Ġal', 'dia', 'Ġpas', 'atu', 'Ġbehar', 'Ġizan', 'Ġz', 'uten', 'Ġgu', 'zt', 'iek', '.', 'ĠHor', 'iet', 'ako', 'Ġzaz', 'p', 'ik', 'Ġtr', 'atu', 'Ġtx', 'arr', 'ak', 'Ġere', 'Ġsal', 'atu', 'Ġz', 'it', 'uz', 'ten', '.', 'ĠHor', 'rez', 'Ġgain', ',', 'Ġbeste', 'Ġbi', 'Ġgaz', 'ter', 'en', 'Ġaur', 'k', 'ako', 'Ġat', 'x', 'ilot', 'ze', 'Ġag', 'ind', 'ua', 'Ġere', 'Ġeman', 'Ġzuen', 'ĠGar', 'zon', 'Ġep', 'a', 'ile', 'ak', ',', 'Ġeta', 'Ġber', 'en', 'Ġbur', 'ua', 'Ġaur', 'ke', 'zt', 'u', 'Ġz', 'uten', 'ĠEsp', 'ain', 'i', 'ako', 'Ġep', 'a', 'ite', 'g', 'ian', '.', 'ĠHam', 'ar', 'ret', 'ik', 'Ġz', 'ort', 'zi', 'Ġes', 'pet', 'x', 'era', 'Ġs', 'art', 'u', 'Ġz', 'it', 'uz', 'ten', ',', 'Ġeta', 'Ġbeste', 'Ġbi', 'Ġbald', 'int', 'z', 'ape', 'an', 'Ġas', 'ke', 'Ġger', 'atu', 'Ġz', 'iren', ';', 'Ġhor', 'iet', 'ako', 'Ġbat', ',', 'Ġ', '10', '.', '000', 'Ġeuro', 'ko', 'Ġber', 'me', 'a', 'Ġord', 'ain', 'd', 'uta', '.', 'Ċ', 'Ċ', 'Esp', 'et', 'x', 'er', 'atu', 'Ġz', 'it', 'uz', 'ten', 'et', 'ako', 'Ġgeh', 'ien', 'ek', 'Ġhil', 'ab', 'ete', 'ak', 'Ġegin', 'Ġdituzte', 'ĠMad', 'ril', 'Ġing', 'ur', 'uan', 'Ġ(', 'Esp', 'ain', 'ia', '),', 'Ġbeh', 'in', '-', 'beh', 'ine', 'an', 'Ġpres', 'o', '.', 'Ġ', '10', '.', '000', 'Ġeta', 'Ġ', '40', '.', '000', 'Ġeuro', 'Ġarte', 'ko', 'Ġber', 'me', 'ak', 'Ġord', 'ain', 'du', 'Ġdituzte', 'Ġbald', 'int', 'z', 'ape', 'an', 'Ġkal', 'era', 'Ġ', 'irt', 'et', 'eko', '.', 'ĠGaz', 'te', 'ek', 'Ġbere', 'k', 'Ġad', 'ier', 'azi', 'Ġdut', 'enez', ',', 'Ġor', 'dea', ',', 'ĠHern', 'an', 'iko', 'ena', 'Ġez', 'Ġzen', 'Ġg', 'isa', 'Ġhor', 'ret', 'ako', 'Ġlehen', 'Ġoper', 'azio', 'a', 'Ġizan', ',', 'Ġez', 'ta', 'Ġaz', 'ken', 'a', 'Ġere', '.', 'Ċ', 'Ċ', 'I', 'zan', 'Ġere', ',', 'Ġ', '200', '5', '.', 'Ġurte', 'an', 'Ġle', 'ge', 'z', 'Ġkan', 'pok', 'ot', 'zat', 'Ġjo', 'Ġz', 'it', 'uen', 'ĠJar', 'rai', '-', 'Ha', 'ika', '-', 'Seg', 'i', 'Ġgaz', 'te', 'Ġer', 'ak', 'unde', 'ak', 'ĠEsp', 'ain', 'i', 'ako', 'ĠA', 'uz', 'ite', 'gi', 'ĠNaz', 'ional', 'ak', '.', 'Ġ', '200', '7', 'ko', 'Ġur', 't', 'arr', 'ile', 'an', ',', 'Ġb', 'err', 'iz', ',', 'ĠEsp', 'ain', 'i', 'ako', 'ĠA', 'uz', 'ite', 'gi', 'ĠG', 'oren', 'ak', 'ĠÂ«', 'terror', 'ista', 'Â»', 'Ġiz', 'end', 'atu', 'Ġz', 'it', 'uen', 'Ġer', 'ak', 'unde', 'ok', ',', 'Ġhor', 'iet', 'an', 'Ġj', 'ard', 'ute', 'a', 'Ġgut', 'xi', 'enez', 'Ġsei', 'Ġurte', 'ko', 'Ġes', 'pet', 'xe', 'aldi', 'arekin', 'Ġzig', 'ort', 'uz', '.', 'ĠG', 'ero', 'zt', 'ik', ',', 'Ġber', 'reh', 'un', 'Ġgaz', 'tet', 'ik', 'Ġg', 'ora', 'Ġat', 'x', 'ilot', 'u', 'Ġdituzte', 'Ġhor', 'reg', 'atik', '.', 'ĠGaz', 'te', 'Ġ', '1', '+', 'ez', '!', 'Ġplata', 'form', 'ak', 'Ġjak', 'inar', 'azi', 'Ġzu', 'enez', ',', 'Ġhor', 'iet', 'ako', 'Ġask', 'ok', 'Ġtort', 'ur', 'ak', 'Ġeta', 'Ġes', 'pet', 'xea', 'Ġpair', 'atu', 'Ġdituzte', '.', 'ĠOr', 'ain', ',', 'ĠHern', 'an', 'iko', 'Ġgaz', 'te', 'ak', 'Ġbez', 'ala', ',', 'Ġep', 'a', 'ik', 'eta', 'Ġno', 'iz', 'Ġe', 'ging', 'o', 'Ġz', 'ain', 'Ġda', 'ude', 'Ġhor', 'iet', 'ako', 'Ġask', 'o', '.', 'Ċ', 'Ċ', 'Â«', 'J', 'ard', 'un', 'Ġmilit', 'ante', 'a', ',', 'Ġj', 'azar', 'ria', 'Â»', 'Ċ', 'Ċ', 'H', 'amar', 'Ġgaz', 'te', 'ak', 'ĠÂ«', 'g', 'az', 'te', 'Ġeta', 'Ġe', 'zk', 'ert', 'iar', 'Ġik', 'us', 'peg', 'it', 'ik', 'Ġindependent', 'ismo', 'a', 'Ġb', 'ultz', 'at', 'ze', 'ag', 'atik', 'Â»', 'Ġep', 'ait', 'uko', 'Ġdituz', 't', 'ela', 'Ġsal', 'atu', 'Ġdu', 'ĠEle', 'ak', 'Ġmug', 'im', 'end', 'uak', ',', 'Ġeta', 'Ġo', 'har', 'atar', 'azi', 'Ġdu', 'Ġg', 'aur', 'Ġeg', 'un', 'Ġere', 'Ġj', 'ende', 'Ġask', 'ok', 'Ġjar', 'rait', 'zen', 'Ġdu', 'ela', 'Ġj', 'azar', 'ria', 'Ġer', 'ak', 'unde', 'Ġso', 'zial', 'Ġze', 'in', 'Ġpolit', 'iko', 'et', 'ako', 'Ġk', 'ide', 'Ġiz', 'ate', 'Ġh', 'uts', 'ag', 'atik', '.', 'ĠHor', 'rel', 'a', ',', 'ĠÂ«', 'ep', 'a', 'ik', 'eta', 'Ġpolit', 'iko', 'Ġgu', 'z', 'ti', 'ak', 'Ġeta', 'Ġesk', 'ub', 'ide', 'Ġz', 'ibil', 'Ġeta', 'Ġpolit', 'iko', 'ak', 'Ġur', 'rat', 'zen', 'Ġdituz', 'ten', 'Ġle', 'ge', 'ak', 'Â»', 'Ġbert', 'an', 'Ġbe', 'her', 'a', 'Ġuz', 'te', 'ko', 'Ġesk', 'atu', 'Ġdu', 'ĠEle', 'ak', '-', 'ek', '.', 'ĠLar', 'un', 'bate', 'an', ',', 'Ġmanifest', 'azio', 'a', 'Ġeta', 'Ġek', 'ital', 'dia', 'Ġe', 'ging', 'o', 'Ġdituzte', 'ĠHern', 'an', 'in', '.', 'Ġ', '19', ':', '00', 'etan', 'Ġizango', 'Ġda', ',', 'Ġher', 'riko', 'Ġpl', 'az', 'an', ',', 'ĠÂ«', 'esk', 'ub', 'ide', 'Ġz', 'ibil', 'Ġeta', 'Ġpolitik', 'oen', 'Ġal', 'de', 'Â».', '</s>']\n",
      "Normalized dyn_tokens example: ['<', 's', '>', 'Z', 'a', 'z', 'p', 'i', 'e', 't', 'a', 'b', 'e', 'd', 'e', 'r', 'a', 't', 'z', 'i', 'u', 'r', 't', 'e', 'a', 'r', 't', 'e', 'k', 'o', 'z', 'i', 'g', 'o', 'r', 'e', 's', 'k', 'a', 'e', 'r', 'a', 'k', 'h', 'a', 'm', 'a', 'r', 'g', 'a', 'z', 't', 'e', 'r', 'e', 'n', 't', 'z', 'a', 't', 'Ċ', 'Ċ', 'H', 'e', 'r', 'n', 'a', 'n', 'i', 'k', 'o', 'g', 'a', 'z', 't', 'e', 'e', 'n', 'a', 'u', 'r', 'k', 'a', 'k', 'o', 'Â', '«', 'e', 'p', 'a', 'i', 'k', 'e', 't', 'a', 'p', 'o', 'l', 'i', 't', 'i', 'k', 'o', 'a', 'Â', '»', 's', 'a', 'l', 'a', 't', 'u', 'd', 'u', 'E', 'l', 'e', 'a', 'k', '-', 'e', 'k', ';', 'l', 'a', 'r', 'u', 'n', 'b', 'a', 't', 'e', 'r', 'a', 'k', 'o', 'm', 'a', 'n', 'i', 'f', 'e', 's', 't', 'a', 'z', 'i', 'o', 'r', 'a', 'd', 'e', 'i', 't', 'u', 'd', 'u', 'Ċ', 'Ċ', 'I', 'r', 'a', 'i', 'l', 'a', 'r', 'e', 'n', '1', '8', 't', 'i', 'k', '2', '8', 'r', 'a', 'b', 'i', 't', 'a', 'r', 't', 'e', 'a', 'n', 'H', 'e', 'r', 'n', 'a', 'n', 'i', 'k', 'o', 'h', 'a', 'm', 'a', 'r', 'g', 'a', 'z', 't', 'e', 'e', 'p', 'a', 'i', 't', 'u', 'k', 'o', 'd', 'i', 't', 'u', 'z', 't', 'e', 'E', 's', 'p', 'a', 'i', 'n', 'i', 'a', 'k', 'o', 'A', 'u', 'z', 'i', 't', 'e', 'g', 'i', 'N', 'a', 'z', 'i', 'o', 'n', 'a', 'l', 'e', 'a', 'n', '.', 'Z', 'a', 'z', 'p', 'i', 'e', 't', 'a', 'b', 'e', 'd', 'e', 'r', 'a', 't', 'z', 'i', 'u', 'r', 't', 'e', 'a', 'r', 't', 'e', 'k', 'o', 'e', 's', 'p', 'e', 't', 'x', 'e', 'z', 'i', 'g', 'o', 'r', 'r', 'a', 'k', 'e', 's', 'k', 'a', 't', 'u', 'd', 'i', 't', 'u', 'z', 't', 'e', 'h', 'a', 'i', 'e', 'n', 't', 'z', 'a', 't', ',', 'S', 'e', 'g', 'i', 'g', 'a', 'z', 't', 'e', 'e', 'r', 'a', 'k', 'u', 'n', 'd', 'e', 'k', 'o', 'k', 'i', 'd', 'e', 'i', 'z', 'a', 't', 'e', 'a', 'l', 'e', 'p', 'o', 'r', 'a', 't', 'u', 't', 'a', '.', 'E', 'l', 'e', 'a', 'k', 'm', 'u', 'g', 'i', 'm', 'e', 'n', 'd', 'u', 'a', 'k', 's', 'a', 'l', 'a', 't', 'u', 'd', 'u', 'j', 'a', 'r', 'd', 'u', 'e', 'r', 'a', 'p', 'o', 'l', 'i', 't', 'i', 'k', 'o', 'e', 't', 'a', 's', 'o', 'z', 'i', 'a', 'l', 'a', 'g', 'a', 't', 'i', 'k', 'e', 'p', 'a', 'i', 't', 'u', 'k', 'o', 'd', 'i', 't', 'u', 'z', 't', 'e', 'l', 'a', 'h', 'a', 'm', 'a', 'r', 'r', 'a', 'k', ',', 'e', 't', 'a', 'm', 'a', 'n', 'i', 'f', 'e', 's', 't', 'a', 'z', 'i', 'o', 'r', 'a', 'd', 'e', 'i', 't', 'u', 'd', 'u', 'H', 'e', 'r', 'n', 'a', 'n', 'i', 'n', 'd', 'a', 't', 'o', 'r', 'r', 'e', 'n', 'l', 'a', 'r', 'u', 'n', 'b', 'a', 't', 'e', 'r', 'a', 'k', 'o', ',', 'a', 't', 'z', 'o', 'i', 'l', 'u', 'n', 't', 'z', 'e', 'a', 'n', 'e', 'g', 'i', 'n', 'd', 'a', 'k', 'o', 'p', 'r', 'e', 'n', 't', 's', 'a', 'u', 'r', 'r', 'e', 'k', 'o', 'j', 'e', 'n', 'd', 'e', 't', 's', 'u', 'a', 'n', '.', 'D', 'a', 't', 'o', 'z', 'e', 'n', 'a', 's', 't', 'e', 'e', 't', 'a', 'n', 'e', 'p', 'a', 'i', 't', 'u', 'k', 'o', 'd', 'i', 't', 'u', 'z', 't', 'e', ',', 'b', 'a', 'i', 'n', 'a', 'e', 'z', 'd', 'a', 'a', 't', 'z', 'o', 'g', 'o', 'i', 'z', 'e', 'k', 'o', 'a', 'g', 'a', 'z', 't', 'e', 'h', 'o', 'r', 'i', 'e', 'n', 'p', 'o', 'l', 'i', 'z', 'i', 'a', 'e', 't', 'x', 'e', 'e', 't', 'a', 'k', 'o', ',', 'e', 's', 'p', 'e', 't', 'x', 'e', 'e', 't', 'a', 'k', 'o', 'e', 't', 'a', 'e', 'p', 'a', 'i', 't', 'e', 'g', 'i', 'e', 't', 'a', 'k', 'o', 'i', 'b', 'i', 'l', 'e', 'r', 'a', '.', 'Ċ', 'Ċ', '2', '0', '0', '9', 'k', 'o', 'm', 'a', 'r', 't', 'x', 'o', 'a', 'r', 'e', 'n', '3', '1', 'n', 'e', 'g', 'i', 'n', 'z', 'u', 'e', 'n', 'E', 's', 'p', 'a', 'i', 'n', 'i', 'a', 'k', 'o', 'P', 'o', 'l', 'i', 'z', 'i', 'a', 'k', 'H', 'e', 'r', 'n', 'a', 'n', 'i', 'e', 't', 'a', 'U', 'r', 'n', 'i', 'e', 't', 'a', 'k', 'o', 'g', 'a', 'z', 't', 'e', 'h', 'o', 'r', 'i', 'e', 'n', 'a', 'u', 'r', 'k', 'a', 'k', 'o', 'o', 'p', 'e', 'r', 'a', 'z', 'i', 'o', 'a', ',', 'B', 'a', 'l', 't', 'a', 's', 'a', 'r', 'G', 'a', 'r', 'z', 'o', 'n', 'e', 'p', 'a', 'i', 'l', 'e', 'a', 'r', 'e', 'n', 'a', 'g', 'i', 'n', 'd', 'u', 'z', '.', 'G', 'o', 'i', 'z', 'a', 'l', 'd', 'e', 'a', 'n', 'e', 'g', 'i', 'n', 'z', 'u', 't', 'e', 'n', 'o', 'p', 'e', 'r', 'a', 'z', 'i', 'o', 'a', ',', 'e', 't', 'a', 'Â', '«', 'e', 'g', 'o', 'e', 'r', 'a', 'o', 's', 'o', 'b', 'o', 'r', 't', 'i', 't', 'z', 'a', 'k', 'Â', '»', 'j', 'a', 's', 'a', 'n', 'b', 'e', 'h', 'a', 'r', 'i', 'z', 'a', 'n', 'z', 'i', 't', 'u', 'z', 't', 'e', 'l', 'a', 'j', 'a', 'k', 'i', 'n', 'a', 'r', 'a', 'z', 'i', 'd', 'u', 't', 'e', 'g', 'a', 'z', 't', 'e', 'e', 'k', '.', 'Z', 'o', 'r', 't', 'z', 'i', 'i', 'z', 'a', 'n', 'z', 'i', 'r', 'e', 'n', 'a', 't', 'x', 'i', 'l', 'o', 't', 'u', 'a', 'k', ',', 'e', 't', 'a', ',', 'e', 't', 'x', 'e', 'k', 'o', 'm', 'i', 'a', 'k', 'e', 't', 'e', 'z', 'g', 'a', 'i', 'n', ',', 'b', 'o', 's', 't', 'e', 'g', 'u', 'n', 'e', 'k', 'o', 'i', 'n', 'k', 'o', 'm', 'u', 'n', 'i', 'k', 'a', 'z', 'i', 'o', 'a', 'l', 'd', 'i', 'a', 'p', 'a', 's', 'a', 't', 'u', 'b', 'e', 'h', 'a', 'r', 'i', 'z', 'a', 'n', 'z', 'u', 't', 'e', 'n', 'g', 'u', 'z', 't', 'i', 'e', 'k', '.', 'H', 'o', 'r', 'i', 'e', 't', 'a', 'k', 'o', 'z', 'a', 'z', 'p', 'i', 'k', 't', 'r', 'a', 't', 'u', 't', 'x', 'a', 'r', 'r', 'a', 'k', 'e', 'r', 'e', 's', 'a', 'l', 'a', 't', 'u', 'z', 'i', 't', 'u', 'z', 't', 'e', 'n', '.', 'H', 'o', 'r', 'r', 'e', 'z', 'g', 'a', 'i', 'n', ',', 'b', 'e', 's', 't', 'e', 'b', 'i', 'g', 'a', 'z', 't', 'e', 'r', 'e', 'n', 'a', 'u', 'r', 'k', 'a', 'k', 'o', 'a', 't', 'x', 'i', 'l', 'o', 't', 'z', 'e', 'a', 'g', 'i', 'n', 'd', 'u', 'a', 'e', 'r', 'e', 'e', 'm', 'a', 'n', 'z', 'u', 'e', 'n', 'G', 'a', 'r', 'z', 'o', 'n', 'e', 'p', 'a', 'i', 'l', 'e', 'a', 'k', ',', 'e', 't', 'a', 'b', 'e', 'r', 'e', 'n', 'b', 'u', 'r', 'u', 'a', 'a', 'u', 'r', 'k', 'e', 'z', 't', 'u', 'z', 'u', 't', 'e', 'n', 'E', 's', 'p', 'a', 'i', 'n', 'i', 'a', 'k', 'o', 'e', 'p', 'a', 'i', 't', 'e', 'g', 'i', 'a', 'n', '.', 'H', 'a', 'm', 'a', 'r', 'r', 'e', 't', 'i', 'k', 'z', 'o', 'r', 't', 'z', 'i', 'e', 's', 'p', 'e', 't', 'x', 'e', 'r', 'a', 's', 'a', 'r', 't', 'u', 'z', 'i', 't', 'u', 'z', 't', 'e', 'n', ',', 'e', 't', 'a', 'b', 'e', 's', 't', 'e', 'b', 'i', 'b', 'a', 'l', 'd', 'i', 'n', 't', 'z', 'a', 'p', 'e', 'a', 'n', 'a', 's', 'k', 'e', 'g', 'e', 'r', 'a', 't', 'u', 'z', 'i', 'r', 'e', 'n', ';', 'h', 'o', 'r', 'i', 'e', 't', 'a', 'k', 'o', 'b', 'a', 't', ',', '1', '0', '.', '0', '0', '0', 'e', 'u', 'r', 'o', 'k', 'o', 'b', 'e', 'r', 'm', 'e', 'a', 'o', 'r', 'd', 'a', 'i', 'n', 'd', 'u', 't', 'a', '.', 'Ċ', 'Ċ', 'E', 's', 'p', 'e', 't', 'x', 'e', 'r', 'a', 't', 'u', 'z', 'i', 't', 'u', 'z', 't', 'e', 'n', 'e', 't', 'a', 'k', 'o', 'g', 'e', 'h', 'i', 'e', 'n', 'e', 'k', 'h', 'i', 'l', 'a', 'b', 'e', 't', 'e', 'a', 'k', 'e', 'g', 'i', 'n', 'd', 'i', 't', 'u', 'z', 't', 'e', 'M', 'a', 'd', 'r', 'i', 'l', 'i', 'n', 'g', 'u', 'r', 'u', 'a', 'n', '(', 'E', 's', 'p', 'a', 'i', 'n', 'i', 'a', ')', ',', 'b', 'e', 'h', 'i', 'n', '-', 'b', 'e', 'h', 'i', 'n', 'e', 'a', 'n', 'p', 'r', 'e', 's', 'o', '.', '1', '0', '.', '0', '0', '0', 'e', 't', 'a', '4', '0', '.', '0', '0', '0', 'e', 'u', 'r', 'o', 'a', 'r', 't', 'e', 'k', 'o', 'b', 'e', 'r', 'm', 'e', 'a', 'k', 'o', 'r', 'd', 'a', 'i', 'n', 'd', 'u', 'd', 'i', 't', 'u', 'z', 't', 'e', 'b', 'a', 'l', 'd', 'i', 'n', 't', 'z', 'a', 'p', 'e', 'a', 'n', 'k', 'a', 'l', 'e', 'r', 'a', 'i', 'r', 't', 'e', 't', 'e', 'k', 'o', '.', 'G', 'a', 'z', 't', 'e', 'e', 'k', 'b', 'e', 'r', 'e', 'k', 'a', 'd', 'i', 'e', 'r', 'a', 'z', 'i', 'd', 'u', 't', 'e', 'n', 'e', 'z', ',', 'o', 'r', 'd', 'e', 'a', ',', 'H', 'e', 'r', 'n', 'a', 'n', 'i', 'k', 'o', 'e', 'n', 'a', 'e', 'z', 'z', 'e', 'n', 'g', 'i', 's', 'a', 'h', 'o', 'r', 'r', 'e', 't', 'a', 'k', 'o', 'l', 'e', 'h', 'e', 'n', 'o', 'p', 'e', 'r', 'a', 'z', 'i', 'o', 'a', 'i', 'z', 'a', 'n', ',', 'e', 'z', 't', 'a', 'a', 'z', 'k', 'e', 'n', 'a', 'e', 'r', 'e', '.', 'Ċ', 'Ċ', 'I', 'z', 'a', 'n', 'e', 'r', 'e', ',', '2', '0', '0', '5', '.', 'u', 'r', 't', 'e', 'a', 'n', 'l', 'e', 'g', 'e', 'z', 'k', 'a', 'n', 'p', 'o', 'k', 'o', 't', 'z', 'a', 't', 'j', 'o', 'z', 'i', 't', 'u', 'e', 'n', 'J', 'a', 'r', 'r', 'a', 'i', '-', 'H', 'a', 'i', 'k', 'a', '-', 'S', 'e', 'g', 'i', 'g', 'a', 'z', 't', 'e', 'e', 'r', 'a', 'k', 'u', 'n', 'd', 'e', 'a', 'k', 'E', 's', 'p', 'a', 'i', 'n', 'i', 'a', 'k', 'o', 'A', 'u', 'z', 'i', 't', 'e', 'g', 'i', 'N', 'a', 'z', 'i', 'o', 'n', 'a', 'l', 'a', 'k', '.', '2', '0', '0', '7', 'k', 'o', 'u', 'r', 't', 'a', 'r', 'r', 'i', 'l', 'e', 'a', 'n', ',', 'b', 'e', 'r', 'r', 'i', 'z', ',', 'E', 's', 'p', 'a', 'i', 'n', 'i', 'a', 'k', 'o', 'A', 'u', 'z', 'i', 't', 'e', 'g', 'i', 'G', 'o', 'r', 'e', 'n', 'a', 'k', 'Â', '«', 't', 'e', 'r', 'r', 'o', 'r', 'i', 's', 't', 'a', 'Â', '»', 'i', 'z', 'e', 'n', 'd', 'a', 't', 'u', 'z', 'i', 't', 'u', 'e', 'n', 'e', 'r', 'a', 'k', 'u', 'n', 'd', 'e', 'o', 'k', ',', 'h', 'o', 'r', 'i', 'e', 't', 'a', 'n', 'j', 'a', 'r', 'd', 'u', 't', 'e', 'a', 'g', 'u', 't', 'x', 'i', 'e', 'n', 'e', 'z', 's', 'e', 'i', 'u', 'r', 't', 'e', 'k', 'o', 'e', 's', 'p', 'e', 't', 'x', 'e', 'a', 'l', 'd', 'i', 'a', 'r', 'e', 'k', 'i', 'n', 'z', 'i', 'g', 'o', 'r', 't', 'u', 'z', '.', 'G', 'e', 'r', 'o', 'z', 't', 'i', 'k', ',', 'b', 'e', 'r', 'r', 'e', 'h', 'u', 'n', 'g', 'a', 'z', 't', 'e', 't', 'i', 'k', 'g', 'o', 'r', 'a', 'a', 't', 'x', 'i', 'l', 'o', 't', 'u', 'd', 'i', 't', 'u', 'z', 't', 'e', 'h', 'o', 'r', 'r', 'e', 'g', 'a', 't', 'i', 'k', '.', 'G', 'a', 'z', 't', 'e', '1', '+', 'e', 'z', '!', 'p', 'l', 'a', 't', 'a', 'f', 'o', 'r', 'm', 'a', 'k', 'j', 'a', 'k', 'i', 'n', 'a', 'r', 'a', 'z', 'i', 'z', 'u', 'e', 'n', 'e', 'z', ',', 'h', 'o', 'r', 'i', 'e', 't', 'a', 'k', 'o', 'a', 's', 'k', 'o', 'k', 't', 'o', 'r', 't', 'u', 'r', 'a', 'k', 'e', 't', 'a', 'e', 's', 'p', 'e', 't', 'x', 'e', 'a', 'p', 'a', 'i', 'r', 'a', 't', 'u', 'd', 'i', 't', 'u', 'z', 't', 'e', '.', 'O', 'r', 'a', 'i', 'n', ',', 'H', 'e', 'r', 'n', 'a', 'n', 'i', 'k', 'o', 'g', 'a', 'z', 't', 'e', 'a', 'k', 'b', 'e', 'z', 'a', 'l', 'a', ',', 'e', 'p', 'a', 'i', 'k', 'e', 't', 'a', 'n', 'o', 'i', 'z', 'e', 'g', 'i', 'n', 'g', 'o', 'z', 'a', 'i', 'n', 'd', 'a', 'u', 'd', 'e', 'h', 'o', 'r', 'i', 'e', 't', 'a', 'k', 'o', 'a', 's', 'k', 'o', '.', 'Ċ', 'Ċ', 'Â', '«', 'J', 'a', 'r', 'd', 'u', 'n', 'm', 'i', 'l', 'i', 't', 'a', 'n', 't', 'e', 'a', ',', 'j', 'a', 'z', 'a', 'r', 'r', 'i', 'a', 'Â', '»', 'Ċ', 'Ċ', 'H', 'a', 'm', 'a', 'r', 'g', 'a', 'z', 't', 'e', 'a', 'k', 'Â', '«', 'g', 'a', 'z', 't', 'e', 'e', 't', 'a', 'e', 'z', 'k', 'e', 'r', 't', 'i', 'a', 'r', 'i', 'k', 'u', 's', 'p', 'e', 'g', 'i', 't', 'i', 'k', 'i', 'n', 'd', 'e', 'p', 'e', 'n', 'd', 'e', 'n', 't', 'i', 's', 'm', 'o', 'a', 'b', 'u', 'l', 't', 'z', 'a', 't', 'z', 'e', 'a', 'g', 'a', 't', 'i', 'k', 'Â', '»', 'e', 'p', 'a', 'i', 't', 'u', 'k', 'o', 'd', 'i', 't', 'u', 'z', 't', 'e', 'l', 'a', 's', 'a', 'l', 'a', 't', 'u', 'd', 'u', 'E', 'l', 'e', 'a', 'k', 'm', 'u', 'g', 'i', 'm', 'e', 'n', 'd', 'u', 'a', 'k', ',', 'e', 't', 'a', 'o', 'h', 'a', 'r', 'a', 't', 'a', 'r', 'a', 'z', 'i', 'd', 'u', 'g', 'a', 'u', 'r', 'e', 'g', 'u', 'n', 'e', 'r', 'e', 'j', 'e', 'n', 'd', 'e', 'a', 's', 'k', 'o', 'k', 'j', 'a', 'r', 'r', 'a', 'i', 't', 'z', 'e', 'n', 'd', 'u', 'e', 'l', 'a', 'j', 'a', 'z', 'a', 'r', 'r', 'i', 'a', 'e', 'r', 'a', 'k', 'u', 'n', 'd', 'e', 's', 'o', 'z', 'i', 'a', 'l', 'z', 'e', 'i', 'n', 'p', 'o', 'l', 'i', 't', 'i', 'k', 'o', 'e', 't', 'a', 'k', 'o', 'k', 'i', 'd', 'e', 'i', 'z', 'a', 't', 'e', 'h', 'u', 't', 's', 'a', 'g', 'a', 't', 'i', 'k', '.', 'H', 'o', 'r', 'r', 'e', 'l', 'a', ',', 'Â', '«', 'e', 'p', 'a', 'i', 'k', 'e', 't', 'a', 'p', 'o', 'l', 'i', 't', 'i', 'k', 'o', 'g', 'u', 'z', 't', 'i', 'a', 'k', 'e', 't', 'a', 'e', 's', 'k', 'u', 'b', 'i', 'd', 'e', 'z', 'i', 'b', 'i', 'l', 'e', 't', 'a', 'p', 'o', 'l', 'i', 't', 'i', 'k', 'o', 'a', 'k', 'u', 'r', 'r', 'a', 't', 'z', 'e', 'n', 'd', 'i', 't', 'u', 'z', 't', 'e', 'n', 'l', 'e', 'g', 'e', 'a', 'k', 'Â', '»', 'b', 'e', 'r', 't', 'a', 'n', 'b', 'e', 'h', 'e', 'r', 'a', 'u', 'z', 't', 'e', 'k', 'o', 'e', 's', 'k', 'a', 't', 'u', 'd', 'u', 'E', 'l', 'e', 'a', 'k', '-', 'e', 'k', '.', 'L', 'a', 'r', 'u', 'n', 'b', 'a', 't', 'e', 'a', 'n', ',', 'm', 'a', 'n', 'i', 'f', 'e', 's', 't', 'a', 'z', 'i', 'o', 'a', 'e', 't', 'a', 'e', 'k', 'i', 't', 'a', 'l', 'd', 'i', 'a', 'e', 'g', 'i', 'n', 'g', 'o', 'd', 'i', 't', 'u', 'z', 't', 'e', 'H', 'e', 'r', 'n', 'a', 'n', 'i', 'n', '.', '1', '9', ':', '0', '0', 'e', 't', 'a', 'n', 'i', 'z', 'a', 'n', 'g', 'o', 'd', 'a', ',', 'h', 'e', 'r', 'r', 'i', 'k', 'o', 'p', 'l', 'a', 'z', 'a', 'n', ',', 'Â', '«', 'e', 's', 'k', 'u', 'b', 'i', 'd', 'e', 'z', 'i', 'b', 'i', 'l', 'e', 't', 'a', 'p', 'o', 'l', 'i', 't', 'i', 'k', 'o', 'e', 'n', 'a', 'l', 'd', 'e', 'Â', '»', '.', '<', '/', 's', '>']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 'sequence': 'list' object cannot be converted to 'PyString'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNormalized dyn_tokens example:\u001b[39m\u001b[38;5;124m\"\u001b[39m, dyn_tokens[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# 2) Map tokens to ids, creating new embeddings as needed\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m batch_ids \u001b[38;5;241m=\u001b[39m \u001b[43maugmenter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens_to_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdyn_tokens\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# list of lists\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch_ids[:\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# 3) Convert to padded tensors for model\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# pad with tokenizer.pad_token_id if you have one; else 0\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 165\u001b[0m, in \u001b[0;36mDynamicAugmenter.tokens_to_ids\u001b[0;34m(self, tokenized_batch)\u001b[0m\n\u001b[1;32m    163\u001b[0m new_tokens \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m uniques \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab]\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# ensure they are created/assigned\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m mapping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_and_assign_new_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Now map sequences\u001b[39;00m\n\u001b[1;32m    167\u001b[0m out_ids \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[8], line 99\u001b[0m, in \u001b[0;36mDynamicAugmenter.add_and_assign_new_tokens\u001b[0;34m(self, new_token_strs)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(to_create), CHUNK):\n\u001b[1;32m     98\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m to_create[i:i\u001b[38;5;241m+\u001b[39mCHUNK]\n\u001b[0;32m---> 99\u001b[0m     pred_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_embeddings_for_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     predicted\u001b[38;5;241m.\u001b[39mupdate(pred_chunk)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Now allocate ids and ensure capacity\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 46\u001b[0m, in \u001b[0;36mDynamicAugmenter._predict_embeddings_for_tokens\u001b[0;34m(self, tokens_list)\u001b[0m\n\u001b[1;32m     43\u001b[0m batch_examples \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: t} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens_list]\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Build surface forms matrix (the zett helper expects hypernet_tokenizer)\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m surfaces \u001b[38;5;241m=\u001b[39m \u001b[43mget_surface_form_matrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokens_list\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pass as list of list? the function in zett returns arrs; adapt if needed\u001b[39;49;00m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypernet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhn_surface_maxlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_to_use\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypernet_tokenizer\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# get first output if returns tuple\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Build source embeddings matrix from current model (concatenate in/out as in example)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m src_emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_input_embeddings()\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata,\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_output_embeddings()\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m     56\u001b[0m ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/MASTER/WiSe25/Lab Rotation/dynamic-tokenization/dynamic_tokenization_311/src/zett/zett/utils.py:681\u001b[0m, in \u001b[0;36mget_surface_form_matrix\u001b[0;34m(tokenizer_or_tokens, maxlen, tokenizer_to_use, padding, verbose)\u001b[0m\n\u001b[1;32m    678\u001b[0m     ids \u001b[38;5;241m=\u001b[39m tokenizer_to_use\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids([\u001b[38;5;28mchr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m token_bytes])\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;66;03m# assume hn tokenizer uses byte pretokenization\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m     ids \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mid \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokenizer_to_use\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;241m>\u001b[39m maxlen:\n\u001b[1;32m    684\u001b[0m     ids \u001b[38;5;241m=\u001b[39m ids[:maxlen]\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'sequence': 'list' object cannot be converted to 'PyString'"
     ]
    }
   ],
   "source": [
    "augmenter = DynamicAugmenter(\n",
    "    model=model,\n",
    "    latxa_tokenizer=latxa_tokenizer,\n",
    "    hypernet=hypernet,\n",
    "    hypernet_tokenizer=hypernet_tokenizer,\n",
    "    cache_limit=50000\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "for i in range(0, len(sentences), BATCH_SIZE):\n",
    "    batch = sentences[i:i+BATCH_SIZE]\n",
    "    print(len(batch))\n",
    "    batch = [s for s in batch if s.strip() != \"\"]\n",
    "    print(len(batch))\n",
    "    examples = [{\"text\": s, \"pretokens\": s.split()} for s in batch]\n",
    "\n",
    "    # 1) Dynamic BPE returns token strings per sentence\n",
    "    dyn_tokens, _, _, _ = dynamic_bpe.tokenize_batch(\n",
    "        batch_examples=examples,\n",
    "        max_nr_merges=30,\n",
    "        mlm=True\n",
    "    )\n",
    "    # dyn_tokens is list[list[str]]\n",
    "    print(\"dyn_tokens example:\", dyn_tokens[0])\n",
    "    # Normalize tokens (remove Ġ, split multi-char into chars)\n",
    "    dyn_tokens = normalize_dynbpe_tokens(dyn_tokens)\n",
    "    print(\"Normalized dyn_tokens example:\", dyn_tokens[0])\n",
    "\n",
    "    # 2) Map tokens to ids, creating new embeddings as needed\n",
    "    batch_ids = augmenter.tokens_to_ids(dyn_tokens)  # list of lists\n",
    "    print(batch_ids[:2])\n",
    "\n",
    "    # 3) Convert to padded tensors for model\n",
    "    # pad with tokenizer.pad_token_id if you have one; else 0\n",
    "    pad_id = latxa_tokenizer.pad_token_id or latxa_tokenizer.eos_token_id\n",
    "    maxlen = max(len(x) for x in batch_ids)\n",
    "    input_ids = torch.full((len(batch_ids), maxlen), pad_id, dtype=torch.long, device=device)\n",
    "    attention_mask = torch.zeros_like(input_ids)\n",
    "    for r, seq in enumerate(batch_ids):\n",
    "        input_ids[r, :len(seq)] = torch.tensor(seq, dtype=torch.long, device=device)\n",
    "        attention_mask[r, :len(seq)] = 1\n",
    "\n",
    "    # 4) Run the model\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    # ... downstream evaluation ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c95eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dynamic Tokenization 3.11",
   "language": "python",
   "name": "dynamic_tokenization_311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
